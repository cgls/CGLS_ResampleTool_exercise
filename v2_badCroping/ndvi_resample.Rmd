---
title: "NDVI Resampling using R-based tools"
author: "Xavier Rotllan-Puig"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output: 
  word_document:
    reference_docx: /Users/xavi_rp/Documents/D6_LPD/LPD/ATBD/LPD_MS_styles.docx
    #toc: true #table of content true
    toc_depth: 3  #up to three depths of headings (specified by #, ## and ###)
    #number_sections: true  #number sections at each table header
    #theme: united  #options for theme
    #highlight: tango  #syntax highlighting style
    #css: style.css   #custom css, should be in same folder. Only for HTML
    #pandoc_args:
    #  - --lua-filter=scholar-metadata.lua
    #  - --lua-filter=author-info-blocks.lua
      
#bibliography: lpd_biblio.bib
#csl: methods-in-ecology-and-evolution.csl
always_allow_html: yes

---


```{r setup, include = FALSE, results='asis'}
library(knitr)
library(pander)
library(captioner)
knitr::opts_chunk$set(echo = TRUE)

```

```{r include = FALSE}
if(Sys.info()[4] == "D01RI1700371"){
  path2data <- "E:/rotllxa/NDVI_resample/NDVI_data"
  path2save <- "E:/rotllxa/NDVI_resample/NDVI_resample"
}else if(Sys.info()[4] == "h05-wad.ies.jrc.it"){
  path2data <- ""
  path2save <- ""
}else if(Sys.info()[4] == "MacBook-MacBook-Pro-de-Xavier.local"){
  path2data <- "/Users/xavi_rp/Documents/D6_LPD/NDVI_data"
  path2save <- "/Users/xavi_rp/Documents/D6_LPD/NDVI_resample"
}else{
  stop("Define your machine before to run LPD")
}

load(file = paste0(path2save, "/v1_withNAs/ResampleResults4Report.RData"))

table_num <- captioner::captioner(prefix = "Table")
fig_num <- captioner::captioner(prefix = "Figure")
#ts <- 0
```


**Abstract**

- Assessment of different methods and R-based tools for resampling Land Products (https://land.copernicus.eu/) at 333m to 1km resolution.
- "Bilinear" is a point-based interpolation method which averages the 4 closest pixels to the centre of the new larger cell. Instead, "Aggregation" is an area-based method which computes averages over all the pixels grouped in the new cell, 9 in this case 
- Both methods give similar and good results compared with unprocessed images at 1km (Pearson's *r*: `r round(rsmpl_df_pearson, 3)` and `r round(rsmpl_df_Aggr_pearson, 3)` for "Bilinear" and "Aggregation" respectively)
- Removing "flagged values" on the resampling process slightly improve the results for both methods assessed, especially for the Aggregation one
- An R-based tool to perform these resampling methods could be developed in order to be shared with the Copernicus Global Land Product portal users to help them with the process of resampling 
- Although such tool might find some limitations related to CPU memory to deal with global NetCDF files, it still might be useful for local to regional studies.



# Introduction

```{r include = FALSE}
fig0_1km <- fig_num(name = "f0_1km_300", caption = "Working NDVI maps at 1km and 333m resolution, respectively")
orig_mapsCat <- paste0(path2save, "/v1_withNAs/ndvi1km_300m_Cat.jpg")

fig_300mErr <- fig_num(name = "f_300mErr", caption = paste0("In red, pixels set as values between 251 and 255 (flagged values) in the NetCDF file at 333m resolution (NDVI original values: ", cuttoff_NA_err, " - 0.9360001)"))
map_300mErr <- paste0(path2save, "/v1_withNAs/ndvi300m_kk.jpg")


```


This is a document to show different options for resampling Land Products (https://land.copernicus.eu/), originally at 333m, to a 1km resolution using R functions and packages. Some comparison of the performance of several methods are also provided.

The analysis is done using a subset of the 10-daily Normalized Difference Vegetation Index (NDVI) global product downloaded from the Copernicus Global Land Product portal (https://land.copernicus.eu/global/products/ndvi). The selected images were taken the 11^th^ August 2019 and the working maps are cut between the coordinates (DD) Xmin = 0, Xmax = 4, Ymin = 40, Ymax = 43. See `r fig_num("f0_1km_300", display = "cite")` for the two working maps at 1km and 333m resolution, respectively. 


![`r fig0_1km`](`r orig_mapsCat`)


In the original NetCDF file downloaded from the portal, some pixels are flagged and they have been assigned values between 251 and 255. They are usually water bodies or NoData, and correspond to NDVI values bigger than `r cuttoff_NA_err`. These pixels can be seen in `r fig_num("f_300mErr", display = "cite")`.

![`r fig_300mErr`](`r map_300mErr`)


There are several approaches to resample data to a coarser resolution. They could be grouped into area-based aggregation methods and point-based interpolation methods (e.g. Bilinear and Nearest Neighbour) and can be applied depending on the data type, etc. 




# Bilinear method. Function *resample()* 

```{r include = FALSE}
fig300m_rsampled1km <- fig_num(name = "f_300m_rsampled1km", caption = "Map at 1km resolution resampled from 333m using the Bilinear approach")
map_300m_rsampled1km <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km.jpg")

figResampleCorr <- fig_num(name = "f_ResampleCorr", caption = "Scatterplot of the observed 1km resolution NDVI image (x-axis) against the resampled one from the 333m resolution image (y-axis) using the Bilinear approach. Also Pearson correlation coefficient (Pearson's *r*)")
ResampleCorr <- paste0(path2save, "/v1_withNAs/resample_correlation.jpg")


figResampleOverpred <- fig_num(name = "f_ResampleOverpred", caption = "Map showing over-predicted pixels after being resampled using the Bilinear approach")
ResampleOverpred <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km_badResamplingHigh.jpg")

figResampleUnderpred <- fig_num(name = "f_ResampleUnderpred", caption = "Map showing under-predicted pixels after being resampled using the Bilinear approach")
ResampleUnderpred <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km_badResamplingLow.jpg")

```


The function *resample()*, from the R package *raster*, interpolates the finer raster using as a reference a coarser raster, in this case the 1km global product. Doing so, the user does not need to expand the final map in a subsequent step after resampling, to match the extent of the 1km global product, filling the space with NoData or any other required value. *resample()* can make the job either using the Bilinear or the Nearest Neighbour (NGB) methods. We will be focused on the former as the NGB method is best used for categorical data interpolation. 

The Bilinear method averages the four closer pixels to the centre of the pixels of the new coarser raster, which is passed as an input to the function *resample()*. The result (`r fig_num("f_300m_rsampled1km", display = "cite")`) is a raster with the same extent and resolution than the coarser input. 


![`r fig300m_rsampled1km`](`r map_300m_rsampled1km`)



The `r fig_num("f_ResampleCorr", display = "cite")` shows quite good correlation between the observed 1km resolution NDVI image (x-axis) and the resampled one to 1km from the 333m resolution image (y-axis) using *resample()* with the Bilinear method. The Pearson correlation coefficient (Pearson's *r*) is `r rsmpl_df_pearson`. Those points observed in `r fig_num("f_ResampleCorr", display = "cite")` with very bad resampling results, either with high over-prediction or high under-prediction, correspond to those close to flagged pixels in the original NetCDF file (i.e. missing or NoData, etc). This flagged pixels have large NDVI values (bigger than `r cuttoff_NA_err`), and this is why they highly influence the average calculation, giving bad resampling results. However, they only represent the `r as.vector(badResamplingHighProp + badResamplingLowProp)`% of total pixels. As seen in `r fig_num("f_ResampleOverpred", display = "cite")` and `r fig_num("f_ResampleUnderpred", display = "cite")`, for over-prediction and under-prediction respectively, much of these pixels are close to the coastline.

![`r figResampleCorr`](`r ResampleCorr`)

![`r figResampleOverpred`](`r ResampleOverpred`)

![`r figResampleUnderpred`](`r ResampleUnderpred`)






# Aggregation method. Function *aggregate()* 

```{r include = FALSE}
fig300m_rsampled1km_Aggr <- fig_num(name = "f_300m_rsampled1km_Aggr", caption = "Map at 1km resolution resampled from 333m using the aggregation approach")
map_300m_rsampled1km_Aggr <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km_Aggr.jpg")

figResampleCorr_Aggr <- fig_num(name = "f_ResampleCorr_Aggr", caption = "Scatterplot of the observed 1km resolution NDVI image (x-axis) against the resampled one from the 333m resolution image (y-axis) using the aggregation approach. Also Pearson correlation coefficient (Pearson's *r*)")
ResampleCorr_Aggr <- paste0(path2save, "/v1_withNAs/resample_correlation_Aggr.jpg")

figResampleOverpred_Aggr <- fig_num(name = "f_ResampleOverpred_Aggr", caption = "Map showing over-predicted pixels after being resampled using the aggregation approach")
ResampleOverpred_Aggr <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km_badResamplingHigh_Aggr.jpg")

figResampleUnderpred_Aggr <- fig_num(name = "f_ResampleUnderpred_Aggr", caption = "Map showing under-predicted pixels after being resampled using the aggregation approach")
ResampleUnderpred_Aggr <- paste0(path2save, "/v1_withNAs/ndvi300m_rsampled1km_badResamplingLow_Aggr.jpg")

```


The aggregation method groups rectangular areas of pixels of the finer resolution image to create a new map with larger cells. To do that, the function *aggregate()* of the package *raster* needs to know the factor of aggregation. In this case, the factor is 3 as it needs to go from 333m to 1 km. In addition, *aggregate()* can perform the calculation using different functions. While the default is the average (*mean()*) it can work also with *modal()*, *max()* or *min()*. One of the main limitations of this function is that the user needs to subsequently expand the resulting map to match with the 1km global product. This step might produce some problems with the resulting maps' extents if further analisys need to be done. `r fig_num("f_300m_rsampled1km_Aggr", display = "cite")` show the resulting map after performing the aggregation and the expansion.


![`r fig300m_rsampled1km_Aggr`](`r map_300m_rsampled1km_Aggr`)



The `r fig_num("f_ResampleCorr_Aggr", display = "cite")` shows quite good correlation between the observed 1km resolution NDVI image (x-axis) and the resampled one to 1km from the 333m resolution image (y-axis) using *aggragate()*. The Pearson correlation coefficient (Pearson's *r*) is `r rsmpl_df_Aggr_pearson`, slightly better that the one got using the Bilinear approach (`r rsmpl_df_pearson`). Similarly to Bilinear, `r fig_num("f_ResampleCorr_Aggr", display = "cite")` shows some very bad aggregated results, which also correspond to those pixels close to flagged pixels in the original NetCDF file (i.e. missing or NoData, etc). However, as more pixels are taken to compute the average (9 in this case, 4 for the Bilinear), it is more likely to found groups with more flagged pixels (bigger than `r cuttoff_NA_err`) close to the coastline than inland. This fact makes that it models slightly better than Bilinear inland, but slightly worst in the coastline, although in general terms the proportion of very bad predictions over the total of pixels for both methods are similar (`r as.vector(badResamplingHighProp + badResamplingLowProp)`% vs. `r as.vector(badResamplingHighProp_Aggr + badResamplingLowProp_Aggr)`%, for Bilinear and Aggregation respectively). This patterns can be seen in `r fig_num("f_ResampleOverpred_Aggr", display = "cite")` and `r fig_num("f_ResampleUnderpred_Aggr", display = "cite")`, for over-prediction and under-prediction respectively.


![`r figResampleCorr_Aggr`](`r ResampleCorr_Aggr`)

![`r figResampleOverpred_Aggr`](`r ResampleOverpred_Aggr`)

![`r figResampleUnderpred_Aggr`](`r ResampleUnderpred_Aggr`)




# Dealing with *flagged values*

"Flagged values" are those pixels in the NDVI images corresponding to water bodies, missing data, errors, etc. They have NDVI values > `r round(cuttoff_NA_err, 4)`, or assigned values in the NetCDF between 251 and 255.

One might want to "remove" them from the resampling process as their big values are highly influencing the averages calculated for all resampling methods assessed in the previous document ("NDVI Resampling using R-based tools"), driving to wrong predictions.

The way to remove them from the resampling, in R-based methods, is transforming them into NA (i.e. No Data). However, R-based functions might deal differently with such NA values in their calculations. On the one hand, they can make the calculations avoiding the NA values. For instance, the mean of four values, say 0.45, 0.45, 0.45 and NA, results in an average of 0.45. On the other hand, in other functions the average of the same four values results in NA if the function does not deal with NAs. The implications of this difference for the resampling exercise are that, in the first case, we have a "resampled value" in the new raster, whereas we do not have any value in the second case (i.e. we have NA for that pixel). 

Therefore, what has been done is to repeat the two resampling approaches shown in the previous sections after substituting the "flagged values" with NA, and check the new maps and Pearson correlation coefficients for the resamplings.



## *Flagged values*: Bilinear method with *resample()* function

```{r include = FALSE}

figResampleCorr_2 <- fig_num(name = "f_ResampleCorr_2", caption = "Comparison of scatterplot of unprocessed (x-axis) against resampled (y-axis) maps, using the Bilinear approach, before and after previously reclassifying NAs. Also Pearson's *r* is shown")


```


*resample()* function does not deal with NA. This means that when there is a "flagged value" in the 333m resolution map, the result will be NA in the resampled map for that cell. As there are not many "flagged values" inland on the working maps at 333m resolution, both the new map at 1km and the Pearson's *r* are quite similar.

There are 1299 more NA pixels ("flagged values") in the new resampled 1km map than in the original unprocessed 1km-resolution image, over a total of c. 75000 pixels corresponding to land (152100 total cells). In addition, there are 856 NA pixels in the unprocessed 1km image which have values in the resampled 1km image due to the fact that their values in the 333m image are not flagged. These differences make not big improvements in the Pearson's *r*, going from 0.9636 to 0.9638, as seen in  `r fig_num("f_ResampleCorr_1", display = "cite")`, without and after reclassifying the "flagged values" into NAs, respectively.


![](`r paste0(path2save, "/v1_withNAs/resample_correlation.jpg")`)


![`r figResampleCorr_2`](`r paste0(path2save, "/resample_correlation.jpg")`)


Despite *resample()* does not deal with NAs, the source code of the function could be modified in order to accept keeping NA values out of the average calculation. Doing so, resampled values would be predicted in those cases where at least one cell of the 333m map has a regular value. However, this would mean that with only one out of nine pixels being a value would drive to have a resampled value. Therefore, this option needs to be taken cautiously.



##  *Flagged values*: Aggregation method with *aggregate()* function
```{r include = FALSE}
figResampleCorr_Aggr_2 <- fig_num(name = "f_ResampleCorr_Aggr_2", caption = "Comparison of scatterplot of unprocessed (x-axis) against resampled (y-axis) maps, using the Aggregation approach, before and after previously reclassifying NAs. Also Pearson's *r* is shown")


```

*aggregate()* function has the option to avoid NAs for the calculation of averages, etc. However, as mentioned in the previous subsection, given that there are not many "flagged values" inland on the working 333m maps, Pearson correlation coefficient does not improve very much, although such improvement is a bit bigger than for the *resample()* option. As seen in  `r fig_num("f_ResampleCorr_Aggr_1", display = "cite")` and `r fig_num("f_ResampleCorr_Aggr_2", display = "cite")`, Pearson's *r* are 0.9756 and 0.9871, without reclassifying the "flagged values" into NAs and doing so, respectively.


![`r figResampleCorr_1`](`r paste0(path2save, "/v1_withNAs/resample_correlation_Aggr.jpg")`)


![`r figResampleCorr_2`](`r paste0(path2save, "/resample_correlation_Aggr.jpg")`)







# Conclusions

The assessment of both methods (Bilinear and aggregation) give similar and good results in general terms. Therefore, in this sense, both can be equally recommended to resample the NDVI products from https://land.copernicus.eu/ at 333m to 1km resolution. This assessment, however, has been made with a small subsample of the images. Therefore, for bigger maps there could be some differences regarding processing time due to the fact that the aggregation method needs to be done in several steps and the average is calculated over a larger number of pixels. Parallel processing should definitely be addressed for resamplings at global scale. 

Observing the plots and Pearson correlation coefficients reported in this document, it seems like the effect of avoiding "flagged values" (after reclassifying them to NA) for the resampling process might slightly improve the results, especially for the Aggregation approach.

An R-based tool to perform the resampling assessed in this document could be easily developed in order to be shared with the Copernicus Global Land Product portal users to help them to process themselves the data sets after being downloaded. A limitation of such tool would be likely related to its high resource dependency in terms of CPU memory to deal with global NetCDF files **(It might be more efficient with tiff images, but I need to check!!)**. But, in any case, it could be useful for smaller subsets of data.


